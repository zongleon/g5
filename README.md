# G5

A toy project to become more familiar with distributed training of large transformer models.
Also to learn about recent biological sequence-based models (DNA, protein).

Currently fine-tuning [ProtT5](https://github.com/agemagician/ProtTrans) to 'translate' (not biologically)
between homologous genes in human and mice. However, this repository also contains various other explorations
into protein and genomic language models, including pre-training my own.
